<h2>Some papers and their key ideas for Deep Reinforcement Learning</h2>
<header class="mini">
<h3>Experience Replay</h3>
<p>Re-utilizing sampled experiences - Efficiency</p>
</header>
<p>
Simulating agent actions inside an environment can be costly and the order in which it happens is correlated to previous actions and states. Algorithms using temporal differences learn slowly and need to experience the same state/action various times to approximate any given action or action-value function correctly. In these cases, storing previous experiences in the form of a tuple \((s_t, a_t, r_t, s_{t+1})\) for later usage can be beneficial.
</p>

<p>
The simplest and most common form of using experience replay is storing \(n\) experience tuples  without updating the model, and after sampling mini batches of those experiences for training, where you can process the experiences \(k\) times, or more common called <i>epochs</i>. Both \(n\) and \(k\) choice have trade offs. For \(n\), lower numbers give your faster updates of the model, while larger numbers ensures more stable (diversity) updates. For \(k\), lower numbers ensures no over-fitting at the cost of slow learning (sample efficiency), while larger numbers increase the sample efficiency but brings the possibility of over-fitting.
</p>
<p>
In the paper <a href="https://link.springer.com/content/pdf/10.1007/BF00992699.pdf">"Self-improving reactive agents based on reinforcement learning, planning and teaching"</a> from 1992 the are already results showing that experience replay can significantly improve the learning speed and sample efficiency. From later 2000, many papers covers the benefits and extensions for experience replay.
</p>

  
<p>[Adam, Sander, Lucian Busoniu, and Robert Babuska.] 
"Experience replay for real-time reinforcement learning control." IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 42.2 (2011): 201-212..
 - <a href="http://www.busoniu.net/files/papers/smcc11.pdf">[ pdf ]</a>
 </p>

<header class="mini">
<h3>Deep Q-Learning</h3>
<p>Successfully usage of Deep Learning parametric models</p>
</header>
<p>
Deep Learning has been a trending topic for supervised learning tasks like detection, classification, segmentation etc. Trying to transfer this success to other areas may present it own challenges.
</p>

<p>
Deep Q-Learning uses an artificial neural network taking a raw state representation as input and outputs its action-values to successfully learn how to play some Atari games. Many ideas of it were not new (In <a href="https://link.springer.com/content/pdf/10.1007/BF00992699.pdf">this</a> paper "Self-improving reactive agents based on reinforcement learning, planning and teaching." from 1992 the same concepts are used but without the expressive results of Deep Q-Learning), but the application using raw pixels from imagery with convolutional layers in network, brought a new breath to Reinforcement Learning with current technologies.
<img class="image fit" src="images/network.svg"></img>
</p>

<p>[Mnih, Volodymyr, et al.] 
"Playing atari with deep reinforcement learning." arXiv preprint arXiv:1312.5602 (2013).
 - <a href="https://arxiv.org/pdf/1312.5602.pdf">[ pdf ]</a>
 </p>
<!--  ////////////////////////////////////// -->

<header class="mini">
<h3>Double Deep Q-Learning</h3>
<p>Decouple action choose and evaluation</p>
</header>
<p>
The standard update step for the parametrized Q-Learning is given by
\[
\theta \leftarrow \theta -\alpha (\underbrace{r_{s,a} + \gamma \max_{a' \in A}Q_{\theta}(s',a')}_{\text{TD target}} - Q_{\theta}(s,a))\nabla_{\theta}Q_{\theta}(s,a)
\]
We can expand the TD target as the following
\[
\begin{align}
\text{TD target} &= r_{s,a} + \gamma \max_{a' \in A}Q_{\theta}(s',a')\\
                     &= r_{s,a} + \gamma Q_{\theta}(s',\arg \max_{a \in A}Q_{\theta}(s', a))
\end{align}
\]
where both the action being chosen and the evaluation of its value depends on the same parameters \(\theta\).
</p>

<p>
The idea of Double Deep Q-Learning is to decouple them using different networks to choose and evaluate the action:
\[
{\text{TD target}}_{\text{double}} = r_{s,a} + \gamma Q_{\theta'}(s',\arg \max_{a \in A}Q_{\theta}(s', a))
\]
Maintaining two networks is costly, hence in its implementation it uses a lagged in time network which is updated at fixed time rates. From a conceptual view point it makes a lot of sense to use a fixed network for evaluation. A parametric model being updated at the same time it is being used for evaluation can propagate error easily, in the other hand for a network fixed in time this behavior is attenuated leading to more stability.
</p>

<p>[Van Hasselt, Hado, Arthur Guez, and David Silver.] 
"Deep reinforcement learning with double q-learning." Thirtieth AAAI Conference on Artificial Intelligence. 2016.
 - <a href="http://papers.nips.cc/paper/3964-double-q-learning">[ pdf ]</a>
 </p>
 
<header class="mini">
<h3>Dueling Network</h3>
<p>Network architecture with separated streams for value and advantage</p>
</header>
<p>

<p>
The dueling architecture for Deep Q-Learning, separates the inference of action-values into two components, the value \(V(s)\) and advantage \(A(s,a)\). We know the equality
\[
Q(s,a) = V(s) + A(s,a)
\]
Separating the neural network parameters in three parts:
<ul>
<li>\(\theta\) - The shared parameters which can be seen as the encoding of the inputs</li>
<li>\(\alpha\) - The value function parameters</li>
<li>\(\beta\) - The advantage function parameters</li>
</ul>
alpha and beta are called the streams of the network. The motivations for such architecture lies in the fact that for some states the actions taken my not be relevant. Having separated streams enforces that \(V(s)\) is updated for any action which leads to improvement for all action-values. Also the concept of shared parameters implies reaching an encoding which is consistent for both streams, which could be extended for others designs.
</p>
<p>
The formula used to calculate \(Q(s,a)\) is slightly different from the original, the justification is that it is an approximation of the original values and the formula with value plus advantage may not be unique because shifting by any constant is equivalent, hence they choose to use a parametrization where the advantage has zero mean, which also improves the stability.
\[
Q_{\theta,\alpha,\beta}(s,a) = V_{\theta,\alpha}(s) + \left( A_{\theta, \beta}(s,a) - \frac{1}{|\mathcal{A}|}\sum_{a' \in \mathcal{A}} A_{\theta,\alpha}(s,a') \right)
\]
</p>

<img class="image fit" src="images/dueling-nn.png"></img>

<p>[Wang, Ziyu, et al.] 
 "Dueling network architectures for deep reinforcement learning." arXiv preprint arXiv:1511.06581 (2015).
 - <a href="https://arxiv.org/pdf/1511.06581.pdf">[ pdf ]</a>
 </p>
 
 <header class="mini">
<h3>Curriculum Learning</h3>
<p>Gradually increasing complexity</p>
</header>
<p>

<p>
A key concept of human learning is starting from simple things and increasing complexity. Our entire education is organized in a way of leveraging previous knowledge. <i>Curriculum Learning</i> is an abstraction of this concept for machine learning. (Some of the)Difficult tasks in reinforcement learning can be broken into smaller and simpler sub-tasks or viewed as steps to achieve the final goal.
</p>

<p>
This concept work extremely well for hierarchical tasks. For example the <a customhref="content/rubiks-cube.html">2x2 Rubik's Cube</a>, where to solving complex configurations implies in being able to solve simpler configurations. The mathematical concept of increasing complexity can be seen as increasing entropy. The curriculum itself can be placed as steps where the entropy \(H(C_{\lambda}) < H(C_{\lambda + \epsilon}), \quad \forall \epsilon > 0\), with \(C_{\lambda}\) the distribution of the curriculum at \(\lambda\) step.
</p>

<p>
Experiments (including ours with the Rubik's cube) shows that learning simple tasks and adapting to increasing complex tasks accelerates the learning, and for some cases a proper curriculum is the difference of learning a given task of not being able to learn anything. This is one case where our intuition of human learning, through sub-sets and re-utilizing previous knowledge to generalize or expand into new knowledge coincides with the machine learning experiments.
</p>


<p>[Bengio, Yoshua, et al.] 
 "Curriculum learning." Proceedings of the 26th annual international conference on machine learning. ACM, 2009.
 - <a href="https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/15972/Bengio%2C%202009%20Curriculum%20Learning.pdf">[ pdf ]</a>
 </p>