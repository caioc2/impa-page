<h2>The Rubik's Cube Problem</h2>

<div class="image">
<img src="images/rc2x2.png"></img>
</div>
<br>
<p>The Rubik's cube is 3D combinatorial puzzle. It was been invented by Ernõ Rubik  in 1974. It's compose of smaller cubes, called cubelets which can rotate together with each cube face. It is one of the most sold toys around the world and is comom to be found it the \(2 \times 2 \) and \(3 \times 3 \) versions.</p>

<p>The \(2 \times 2 \) cube(the one in the opening image) is far easier to be solved than its \(3 \times 3 \) counterpart. Solving the cube implies in applying a series of face rotations until each face have the same single color. The \(2 \times 2 \) version have a state space of approximated \(3.6 \times 10^6 \) states while the \(3 \times 3 \) have aproximated \(4.3 \times 10^{19}\) states. Various challenges are presented by these puzzles, among them are:
</p>

<ul>
<li>Discrete action and state space</li>
<li>Unknown direct algorithm to optimally solve it, yet there are algorithms using search and other techniques to solve it optimally or very close to it.</li>
<li>Compact representation of state space is hard, for example our representation of \(2 \times 2 \) space is 2^144, which is approx. 2.2e43 while the state space is much smaller 3.6e6.</li>
<li>Sparse reward system. It's not trivial to evaluate how good or close to the solution one configuration is. Given that positive reward is only assigned when the cube is solved.</li>
</ul>

<p>These make it a hard problem, which still needs a good solution. One of the best know solutions using RL is <a href="https://arxiv.org/abs/1805.07470">Solving the Rubik's Cube Without Human Knowledge</a> which uses a neural network to guide an informed search using Monte Carlo Tree Search(MCTS) and Breadth First Search(BFS). As much as it is interesting by being able to solve almost every \(3 \times 3 \) cube with results close to other search algorithms using various Rubik's cube domain knowledge and group theory, it stills disappointing in one sense: It needs search algorithms after the training, consuming a lot of time and exploration before reaching the solution. Even that the MCTS explore far less states than the traditional search algorithms like DFS and BFS, the need for using it together with BFS points that maybe the search is doing the hard work and not the RL itself.</p>

<p>One attempt of reproducing the results  of <a href="https://arxiv.org/abs/1805.07470">Solving the Rubik's Cube Without Human Knowledge</a> was made in <a href="https://medium.com/datadriveninvestor/reinforcement-learning-to-solve-rubiks-cube-and-other-complex-problems-106424cf26ff">Reinforcement Learning to solve Rubik’s cube (and other complex problems!)</a>, yet the results presented by the original authors couldn't be reproduced, be it by lack of implementation details in the original paper, by capping the allowed search time/nodes or less training than the original.
So far the later also tried the method on \(2 \times 2 \) cubes and it couldn't consistently solve all of the \(2 \times 2 \) cubes. Its results using MCTS would need 50 moves or more, and using MCTS + BFS around 14 moves. Again it reinforces the idea of the search doing the hard work for solving the cube.</p>

<h3>Our Approach</h3>

<p>Solving the cube can be think of 
<blockquote>"At each step chose the move that decreases the number of following moves needed to get to the solved state".</blockquote>
</p>


<p>As much as we don't know how many moves away one state is from solved, this thinking implies that: <i>Any cube state that is \( N \) moves away from the solution necessarily needs to pass through states which are \(N-1\), \(N-2\), \( \dots\), 1 moves away from the solution</i>.

<p>This is a great thing, we can decompose our solve process backward and create a <a href="https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/15972/Bengio%2C%202009%20Curriculum%20Learning.pdf?sequence=1&isAllowed=y">Learning Curriculum</a>. The main idea is to teach the agent to solve cubes starting with 1 move away and increasing it till N moves. This solves the problem of sparse rewards, or not being able to give rewards/classify intermediate states until reaching the solution.</p>

<p>Our criteria for this curriculum were:
<ul>
<li>Solve rate of the cubes with \(k\) moves away from solved.</li>
<li>Solve length rate - \(ck\) moves needed to solve \(k\) scramble depth.</li>
</ul>
</p>
<p>We don't know how to generate cubes with <b>exact</b> \(k\) moves away from solution, but applying a random sequence of \(k\) moves to a solved cube guarantees that this cube is \(k\) or less moves away from solved, which is good enough for our needs.
</p>

<h3>Results</h3>

<p>After training for about 3 hours on a notebook CPU we were able to solve the \(2 \times 2 \) cube with any scramble depth with average solve length of ~26 moves, within a day of training we were able to achieve an average solve length of ~17 moves which is close to the gods number for \(2 \times 2 \) cube which is 14 quarter turn moves.</p>

<div class="image">
<img src="images/2x2.png"></img>
</div>

<p>The artificial neural network used was pretty simple, 2 hidden layers with 512 units each. You can test the DRL solver online <a href="https://caioc2.github.io/RSolver/game/">here</a> or watch the video:
</p>

<div class="video-container">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/TfZp3LCC-uM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
