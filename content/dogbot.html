<header class="mini">
<h2>DogBot</h2>
<p>Experimenting reinforcement learning on a playable character</p>
</header>

<h3>High-Level character control</h3>

<p>Reinforcement learning have been a trending topic recently with the Deep Learning techniques. Various interesting applications have been made from playing games (Atari, Go, StarCraft, etc) to controlling torque-actuators for motion control in robotic simulations (biped run, robotic hand solving Rubik's cube, etc). Here we want to experiment with high level controls of a character, which already have its animations and controller, to do simple tasks like collect or fetch an object.
</p>

<p>Our chosen character is a dog (we will call it DogBot for now). It can stand, walk, trot, run, jump and crouch. We train it to select between those actions at each instant \(t_i\) to accomplish a desired task. More details on the basic animations and controls can be found <a href="anim.html">here</a>.
</p>

<p>With this setup, our later goal is to have some basic trained behaviors which can be used by a higher level "brain" (either learned or heuristic based) to interact with players in a virtual environment. This approach resembles a hierarchical learning spliting complex behaviors into a set of simple behaviors, but instead of learning every single component, it can use heuristic behaviors and traditional animation and character controller together with learning.</p>

<h3>Environment</h3>

<p>Our enviroment is a Unity scene where our character can act and interact with objects. It consists of a square gray plane \(110\times 110\)m with a white border of \(1\)m diameter, which <i>visualy</i> delimits the area of interest. While it is possible to walk past this area, if the agent being trained go past it we consider as it reached a final state (game over) leading to a reset or restart.</p>

<p>Inside our training area we can have the following objects:
<ul>
	<li> Collectibles:
		<ul>
			<li>(Simple geometry) Cubes with \(1\)m edges</li>
			<li>(Complex geometry) Coins with \(1.5\)m diameter</li>
		</ul>
	</li>
	<li>Standing pad: a painted circle in the ground delimiting a region with \(5\)m diameter.</li>
</ul>
One or many of these objects can be found in a specific scenarion for training. We will specify their number, positioning and any other relevant detail for each training section when relevant.
</p>

<p>In general, the choice for big objects is mainly due to the use with visual observations where the size of input image is downsampled to \(84\times 84\) pixels and hence the need of it being big enough to appear in the downsampled input image. One important aspect of the enviroment for the later goal is that what is seen by the agent, for example a big pink cube, is not necessarily visible or the same seen by another player. We can create complex behaviors using simple geometries for our agent while complex geometries and totaly different context is seen by the player and therefore being able to use it in interesting ways to create interactions between the agent(the dogBot) and the player(a humam being).</p>

<h3>Tasks</h3>

<p>The agent task can be put as a mobility task, given a stimulus it needs to locomote to a target point. Inside this general task we have our specialized tasks:
<ul>
	<li>Collecting an object: reach the object position, when the agent colides with it the object is removed from the scene, i.e., collected.</li>
	<li>Reach and stay: reach the standing pad and stay inside it.</li>
	<li>Fetch: reach the object and go back to its initial position. It also can be thought as reaching two objects, for example, the stick and then who threw it.</li>
</ul>

While these can all be cast as essentially the same task, their difference comes of how they are modeled inside the virtual environment and which information is available to the agent to complete them. Nevertheless, they are practical examples of mobility tasks.
</p>

<h3>Agent</h3>

<p>We breifly described which actions or agent (DogBot) can perform with its standard animations and character controller. Yet the most important is what it observes to take actions and how both the observations and actions are encoded.</p>
<p>Before entering the encoding itself it's important to distinguish between the kinds of observations in a higher level:
	<ul>
		<li>Completely observable: all information from the environment is available.</li>
		<li>Partially observable: some of the information from the environment is available.</li>
	</ul>
One example as why we call it <i>completely observable</i> is: </p>
<cite>
Imagine we want our agent to fetch a stick which is behind him, using information such as "direction to the target" or "target position" tells our agent it is behind, even if it is not inside its vision field, as a result, no matter where the stick is, this kind of feature  always gives the agent precise information.
</cite>

<p>In the other hand, following the same example, if we use only the visual observation of what the agent is seeing (a 2D image from its vision cone), depending on where the stick is partial or no relevant information will be available.
</p>
<p>Now we have the two encoding for our observations:
<ul>
	<li>Vector observation: complete observable, hand-crafted features.
		<ul>
			<li>Direction to target: \( (x,y,z)\)</li>
			<li>TODO</li>
		</ul>
	</li>
	<li>Visual observation: partially observable, downsampled from the original rendered image, 3rd person camera-like.
		<ul>
			<li>2D image:  matrix \(V_{84\times 84}\)</li>
		</ul>
	</li>
</ul>
</p>

<p>It is important to note that while each of our encoding sits on a different category, one could have a mix of both observations. Also, being completely or partially observable is not bind to the kind of encoding but the informations passed. For example, in a 2D chess game, if our observation encoding where a view-from-top of the entire board, it would be completely observable.</p>

TODO: action encoding

<h3>Training</h3>

TODO: mlagents setup etc

<h4>Experiments</h4>

The first is an implementation that resembles the <a href="https://blogs.unity3d.com/pt/2018/10/02/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit/">Unity Puppo, the Corgi</a> where vector observation with informations such as, speed, direction, direction to target and others are used to describe the scene.





<figure style="text-align:center;">
<img class="image 25%" src="../images/ch-end.jpg">
</figure>


idea - high level character control
idea - basic learned actions controlled by a bigger "brain"
basic input -  forward, backward, left, right, jump crouch

training
	- collect pink objects
		- binary reward {0,1}
	- reach stand pink pads
		- inside time reward
	- visual observation
	- forward bias
	- continuous action space (-1,1)
		- network output scaling
	- discrete action space {backward, stand, walk, trot, run}, {left, straight, right}
		- forward bias on reward
	- puppo like
		- high level discrete actions
		- vector observations of direction, distance, borders and velocity
	- environment
		- difficult
			- # items
			- size
			- possible actions
	- reward
		- shape behavior
			- binary
			- increasing
			- time penalty
			- action penalty
		- forward bias
		- difficult to attain the desired end result
	- result
		- puppo like
			- works good, "complete" information of ambient
		- visual observation
			- partial information
			- movement tied to vision, lack of attention
			- independent action branches, hard to train together
			- nonoptimal behavior, forward overfit
		- generalization
			- object shape (because of color?)
			- area size
	- network archtecture
			
	