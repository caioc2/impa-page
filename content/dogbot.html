<header class="mini">
<h2>DogBot</h2>
<p>Experimenting reinforcement learning on a playable character</p>
</header>

<h3>High-Level character control</h3>

<p>Reinforcement learning have been a trending topic recently with the Deep Learning techniques. Various interesting applications have been made from playing games (Atari, Go, StarCraft, etc) to controlling torque-actuators for motion control in robotic simulations (biped run, robotic hand solving Rubik's cube, etc). Here we want to experiment with high level controls of a character, which already have its animations and controller, to do simple tasks like collect or fetch an object.
</p>

<p>The chosen character is a dog (call it DogBot for now). It can stand, walk, trot, run, jump and crouch. It was trained to select between those actions at each instant \(t_i\) to accomplish a desired task. More details on the basic animations and controls setup can be found <a href="anim.html">here</a>.
</p>

<p>With this setup, the later goal is to have some basic trained behaviors which can be used by a higher level "brain" (either learned or heuristic based) to interact with players in a virtual environment. This approach resembles a hierarchical learning splitting complex behaviors into a set of simple behaviors, but instead of learning every single component, it can use heuristic behaviors and traditional animation and character controller together with learning.</p>

<h3>Motivation</h3>

<p>While there are plenty of successful cases of low-level training</humam muscle ref>, developing such learning environments and characters demands a lot of additional work and fine tuning, yet the result may not be suitable for artistic purposes</example>. </p>

<p>The mixed use of heuristic, machine learning and traditional animations simplifies both the learning process and the artistic control over the final result. It gathers the best of each world: the traditional animation and heuristic gives the artistic control over the set of actions whereas the machine learning gives the possibility of the agent generalizing for many situations.</p>

<p>It differs from the other approaches using high level control such as the Atari</ref needed> games in its fundamental principle: it is not solving a existing game, but building its own game environment which gives the freedom of balancing between the learning difficult (generalization) and the control over the actions (static heuristic and animations).</p>

<p>

<h3>Environment</h3>

<p>The environment is a Unity scene where the character can act and interact with objects. It consists of a square gray plane \(110\times 110 m\) with a white border of \(1 m\) diameter, which <i>visually</i> delimits the area of interest. While it is possible to walk past this area, if the agent being trained go past it a final state (game over) is reached leading to a reset or restart.</p>

<div class="image fit" style="text-align: center">
<img src="images/env.png">Top View from environment</img>
</div>

<p>The training area can contain the following objects:
<ul>
	<li> Collectibles:
		<ul>
			<li>(Simple geometry) Cubes with \(1 m\) edges</li>
			<li>(Complex geometry) Coins with \(1.5 m\) diameter</li>
		</ul>
	</li>
	<li>Standing pad: 
		<ul>
			<li>A painted circle on the ground delimiting a region with \(5 m\) diameter.</li>
		</ul>
	</li>
</ul>
One or many of these objects can be found in a specific scenario for training. Their count, positioning and any other relevant detail will be specified for each training section later.
</p>

<div class="image fit" style="text-align: center">
<img src="images/objects.png" >Scene objects</img>
</div>

<p>In general, the choice for big objects is mainly due to the use with visual observations where the size of input image is down-sampled to \(84\times 84\) pixels and hence the need of it being big enough to appear in the down-sampled input image.</p>

<div class="image fit" style="text-align: center">
<img src="images/downsampled.png" >(Left) full resolution, (Right) down-sampled visual observation</img>
</div>

</p> One important aspect of the environment for the later goal is that what is seen by the agent, for example a big pink cube, is not necessarily visible or the same seen by a player. Complex behaviors can be created using simple geometries for the agent while complex geometries and totally different context is seen by the player. Therefore, it enables one to use it in interesting ways to create interactions between the agent(the dogBot) and the player(a human being).</p>

<!-- TODO <img src="">Diff views illustration</img> -->

<p>A form of classifying an environment is relative to how observable it is:
	<ul>
		<li>Completely observable: 
			<ul>
				all information from the environment is available.
			</ul>
		</li>
		<li>Partially observable:
			<ul>
				some of the information from the environment is available.
			</ul>
		</li>
	</ul>
This classification is intrinsic related to how the agent senses its surrounding.
One example of a <i>completely observable</i> environment is: </p>
<p>
<blockquote>
Assuming the previous environment without obstacles and with a single collectible object as goal. If the observations are set as the agent position and direction, the direction to target and target position. It is a completely observable configuration in the sense that independent of the agent state (position and direction) it always have complete information  of itself and the target to complete its goal. Indeed one could even write a heuristic to solve this simple task.
</blockquote>
</p>

<p>In the other hand, following the same example, if visual information is used, (i.e, a 2D image from the agent's vision cone) instead of the aforementioned observations, depending on where the agent/target is, it may be partially or not visible at all. In this case the sensing of the agent varies depending of its state and it is always a partial view of the environment which may or not contain relevant information. This differentiation of how the environment is perceived and how much information is available per observation is an important component of learning performance. 
</p>

<h3>Tasks</h3>

<p>The agent task can be put as a mobility task, given a stimulus it needs to move towards the target point. Inside this general task three specialized tasks are implemented:
<ul>
	<li>Collecting an object:
		<ul>reach the object position, when the agent collides with it the object is removed from the scene, i.e., collected.</ul>
	</li>
	<li>Reach and stay: 
		<ul>reach the standing pad and stay inside it.</ul>
	</li>
	<li>Fetch: 
		<ul>reach the object and go back to its initial position. It also can be thought as reaching two objects, for example, the stick and then who threw it.</ul>
	</li>
</ul>

While these can all be cast as essentially the same task, their difference comes of how they are modeled inside the virtual environment and which information is available to the agent to complete them. That said, they are practical examples of mobility tasks.
</p>

<h3>Agent</h3>

<p>The agent is an entity abstraction which is itself a <i>behavior policy</i>. Nevertheless, when referred without a specific policy it can be imagined as an entity with sensors (observations) and actuators (actions) where a policy can be plugged in, in programming it would be equivalent to an <i>interface</i>. The agent(DogBot) had its actions already briefly described with its standard animations and character controller. Yet, the most important is what it observes to take actions, how both the observations and actions are encoded and how the associated rewards are distributed.</p>

<h4>Character controller</h4>

<p>The Unity character controller is the lower level hierarchy of actions and can be considered as the intrinsic of the agent's actuators. It controls the velocity, turn speed, gravity and other effects of the actuator (the character body properties). The list of the parameters used in the training are in the <a href="#char-table">experiments section</a>, while in the following there is a figure of those parameters inside Unity.</p>

<div class="image 50%" style="display: block; text-align: center; margin-left: auto; margin-right: auto;">
	<img src="images/char-control.png" style="margin-left: auto; margin-right: auto;">Character controller parameters</img>
</div>

<h4>Observations</h4>

<p>An observation is any kind of sensing of the agent and environment state. Those need to be encoded by numbers that serves as input to the behavior policy which output actions.</p>

<p>The observations and its encoding are the following:
<ul>
	<li>Vector observation: complete observable, hand-crafted features.
		<ul>
			<li>Normalized direction to target: \( d_{\text{target}} = (x,y,z), \quad \lVert d_{\text{target}} \rVert_{2} = 1\)</li>
			<li>Normalized distance to border: \(d_{\text{border}} = (x,y)\), \(\lVert d_{\text{border}} \rVert_{\infty} < 1 \rightarrow \text{inside}, \quad \lVert d_{\text{border}}\rVert_{\infty} \geq 1 \rightarrow \text{outside}\)</li>
			<li>Linear velocity: \( v_{\text{linear}} = (x,y,z) \text{m/s}\)</li>
			<li>Angular velocity: \( v_{\text{angular}} = (x, y, z) \text{rad/s}\)</li>
			<li>Normalized agent forward direction: \( d_{\text{forward}} = (x, y, z), \quad \lVert d_{\text{forward}} \rVert_{2} = 1\)</li>
			<li>Normalized agent up direction: \( d_{\text{up}} = (x, y, z), \quad \lVert d_{\text{up}} \rVert_{2} = 1\)</li>
			<li>Agent local position (agent's position referent to its transformation matrix): \( p_{\text{local}} = (x,y,z) \)
		</ul>
	</li>
	<li>Visual observation: partially observable, down-sampled from the original rendered image, 3rd-person-like camera aligned with agent forward direction.
		<ul>
			<li>2D image:  matrix \(I_{84\times 84}(r,g,b)\)</li>
		</ul>
	</li>
	<li>Mixed observation: partially observable, down-sampled from the original rendered image, 3rd-person-like camera possible unaligned with agent forward direction.
		<ul>
			<li>2D image:  matrix \(I_{84\times 84}(r,g,b)\)</li>
			<li>Local normalized camera forward direction: \( d_{\text{camera}} = (x,y,z), \quad \lVert d_{\text{target}} \rVert_{2} = 1\)
		</ul>
	</li>
</ul>
</p>

<p><i>Note that being completely or partially observable is not bind to the kind of encoding but the information passed. For example, in a 2D chess game, if the observation encoding were a view-from-top 2D image of the entire board, it would be completely observable.</i></p>

<div class="image fit" style="text-align: center">
<img = src="images/blackbox-diag.svg">Abstraction of observation (input), action (output), model (policy) and the agent.</img>
</div>

<p>The first encoding takes a total of \(20\) numbers (floats) as observation. These were calculated from the agent/environment state and are much like as the observations taken by <a href="https://blogs.unity3d.com/pt/2018/10/02/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit/">Puppo, the Corgi</a>, which is a demo made by Unity. There are some differences to fit our modeling. First, the <i>Normalized distance to the border</i> is added, because it is relevant in an unbounded environment to keep the agent inside the desired area. Last all the joint angles and torque information were removed. Puppo works on a low-level control of the joint angles and torque while this new agent works on a high-level control with animations. Those differences are related to <i>how the agent senses itself</i>, while how it senses the rest of the environment and its target remained the same.</p>

<p>Next, the visual encoding consists of \( 84\times 84 \times 3 = 21168\) numbers (floats), in a 2D RGB image. This image is the direct rendering of the agent's view. This kind of observation is interesting for many real world applications. yet it is much more complex and less complete than the first encoding in \(20\) numbers because only partial information can be inferred from it. In spite of that, it can be more general in some aspects. For example if a variable number of collectibles is admitted in the environment, the first encoding would not be able to handle that variation, but using a 2D image the agent could deal with many objects (because the image is already a partial view of the environment).</p>

<p>Finally, the third encoding adds freedom to the camera forward look direction and hence this information is passed with a 3D vector (local camera direction) together with the 2D RGB image.</p>

<p>One differentiation about those observations is: Despite the 2D image drawbacks, it is an agent self-contained sensing while the vector observation used in the first encoding need access of the underlying environment model to be processed and fed to the agent. The third encoding, regardless of using mixed observations still self-contained.</p>

<h4>Actions</h4>

<p>The DogBot's actuator(s) is a character inside Unity. It is composed of various animations and a controller (with blend trees and alikes) which receives four parameters controlling the X-Axis velocity, the Y-Axis rotation and booleans jump/crouch.</p>

<p>For the agent's action encoding two schemes were used continuous and discrete:
<ul>
	<li>Continuous action space:
		<ul>
			<li>Forward and backward movement: \(\in [-1,1]\)</li>
			<li>Steering left and right: \(\in [-1,1]\)</li>
			<li>Jump: \(j \in [-1,1], \quad j > j_0\)</li>
			<li>Crouch: \(c \in [-1,1], \quad c > c_0\)</li>
		</ul>
	</li>
	<li>Discrete action space:
		<ul>
			<li>Forward and backward movement: \(\{\text{backward, none, walk, trot, run}\}\)</li>
			<li>Steering: \(\{\text{left, none, right}\}\)</li>
			<li>Jump: \(\{\text{true, false}\}\)</li>
			<li>Crouch: \(\{\text{true, false}\}\)</li>
		</ul>
	</li>
</ul>
Each bullet is a action branch, and can be choose simultaneously. For the case of Jumping/Crouching, despite being separated branches, priority is given to Jump action over the Crouch as it is not physically possible to do both at the same time. While the encoding for actions are arbitrary, they reflect the ranges of the Unity character controller such as <i>max velocity</i> and <i>turn speed</i>, which are configurable but from the agent's perspective are intrinsic to its actuator.
</p>

<h4>Reward</h4>

<p>The entire universe of reinforcement learning is based on encouraging the best behavior through rewards (much like it is done when teaching a trick to a pet), in other words, rewarding good actions according. It can also be posed as a optimization problem of maximizing the total reward (a short introduction to this matter can be found <a href="wrap/intro-rl.html">here</a>).</p>

<p>In this scenario two perspectives a brought up: developing a good reward signal is the key to being able to solve this optimization problem, yet most of times it is not as easy to qualify a given action individually, but only the final outcome of a sequence of actions. In theory, even for the cases where only the final outcome is rewarded, in the limit after many (infinite) experiences it would be possible to learn a optimal behavior policy. Sadly in the real world, limited resources are available be it time, number of experiences, and so on. Hence, the art in learning good policies lies in modeling good rewards and good algorithms as much as possible. <i>Here good performance means reduced time and sample complexity.</i></p>


<p>The DogBot agent experiments with  types of reward:
<ul>
	<li>Per action reward:
		<ul>Positive reward is assigned if the agents approximates of the target, negative reward is assigned if it distances itself from it. The formula used in this case is \(r = 0.01 (v_{\text{linear}} \cdot d_{\text{target}})\)</ul>
	</li>
	<li>Sparse reward: 
		<ul>Positive reward \(+1.0\) is given when the agent reach its destination, i.e., the collectible.</ul>
	</li>
</ul>
In all cases, leaving the training area leads to a negative reward of \(-1.0\) ending the episode. Also a small negative reward \(-0.0005\) is given at each time step \(t_i\). It was chosen to be close to \(-\frac{1}{\text{# steps}}\) so the accumulated penalty would not saturate the total reward signal. This is widely used as a time penalty to stimulate the completion of a task in the shortest time.
</p>

<p>It's also important to classify these rewards in another way. The first reward needs a broader knowledge of the environment to be calculated depending both of the agent and environment underlying state. In the other hand, the second could be completely assigned with the agent's sensing. This is interesting because it resembles the agent self-contained property and have implications in the learning performance and generalization. For obvious reasons the later is expected to be harder.</p>

<h3>Training</h3>

<p>The tools used for training were Unity and its machine learning framework <a href="https://unity3d.com/machine-learning/">ML-Agents</a>. All experiments use the Proximal Policy Optimization (<a href="https://openai.com/blog/openai-baselines-ppo/">PPO</a>).</p> 

<p>Each experiment took between \(16\) to \(30\) hours to complete the training process, not accounting for many "pre-experiments" and the generation of results using the test scene. For this reason, the training were run only once and across different machines. While some insight can be taken from the results, it may not be accurate to take conclusions from single experiments (even that they usually tend to converge to the same solution across multiple runs).</p>


<p>The ML-Agents framework exposes some parameters related to it's built-in models (network architecture), they will be specified inside the <a href="#exp_sec">experiment section</a>. Other specific details of the network itself are omitted because they are the default of ML-Agents and can be found in their <a href="https://github.com/Unity-Technologies/ml-agents/">documentation and code</a>. <i>This choice was made because the focus is on the development of the agent, environment and reward signal instead of network architecture.</i>
</p>



<h4 id="exp_sec">Configuration</h4>

<p>The baseline is a implementation that resembles the <a href="https://blogs.unity3d.com/pt/2018/10/02/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit/">Unity Puppo, the Corgi</a> where vector observation with information such as, speed, direction, direction to target and others are used to describe the scene. While the given environment has many differences from the aforementioned, it is a good starting point  to compares results qualitatively.</p>
<!--
<ul>
	<li>Puppo like obs and reward</li>
	<li>Puppo like obs sparse reward</li>
	<li>Vis encode and puppo like reward</li>
	<li>Vis encode sparse reward</li>
	<li>Multiple objects, vis encode, sparse</li>
	<li>Multiple objects, vis encode, sparse, pre-trained puppo like</li>
	<li>Vis encode, Standing pad</li>
	<li>Vis encode, Camera movement detached from forward direction</li>
</ul>
-->
<p>All experiments uses the following parameters:

<table id="char-table">
	<tr><th colspan="3" style="text-align: center"><font size="+2">Character Controller</font></th></tr>
	<tr>
		<th>Name</th>
		<th>Value</th>
		<th>Description</th>
	</tr>
	<tr>
		<td>Moving turn speed</td>
		<td>\(45.0\) \(deg/s\)</td>
		<td>Turn speed when not stationary</td>
		</td>
	</tr>
	<tr>
		<td>Stationary turn speed</td>
		<td>\(30.0\) \(deg/s\)</td>
		<td>Turn speed when stationary</td>
	</tr>
	<tr>
		<td>Jump power</td>
		<td>\(5.0\) \(m/s\)</td>
		<td>Vertical velocity applied when jumping</td>
	</tr>
	<tr>
		<td>Forward Velocity</td>
		<td>\(9.0\) \(m/s\)</td>
		<td>Maximum forward velocity</td>
	</tr>
	<tr>
		<td>Backward Velocity</td>
		<td>\(2.0\) \(m/s\)</td>
		<td>Maximum backward velocity</td>
	</tr>
	<tr>
		<td>Gravity multiplier</td>
		<td>\(1.0\)</td>
		<td>Multiplier for gravity simulation</td>
	</tr>
	<tr>
		<td>Anim speed multiplier</td>
		<td>\(1.0\)</td>
		<td>Multiplier for animation time scale</td>
	</tr>
</table>
</p>

<p>
<table>
	
	<tr><th colspan="3" style="text-align: center"><font size="+2">Training Configuration</font></th></tr>
	<tr>
		<th>Name</th>
		<th>Value</th>
		<th>Description</th>
	</tr>
	<tr>
		<td>batch size (Continuous)</td>
		<td>\(4096\)</td>
		<td>Number of samples used for each optimization step for continuous action space.</td>
		</td>
	</tr>
	<tr>
		<td>batch size (Discrete)</td>
		<td>\(256\)</td>
		<td>(TODO: validate)Number of samples used for each optimization step for discrete action space.</td>
		</td>
	</tr>
	<tr>
		<td>buffer size</td>
		<td>\(40960\)</td>
		<td>Number of samples collected for each policy update.</td>
		</td>
	</tr>
	<tr>
		<td>hidden units</td>
		<td>\(512\)</td>
		<td>Number of neurons per hidden layer.</td>
	</tr>
	<tr>
		<td>num layers</td>
		<td>\(2\)</td>
		<td>Number of hidden layers used for the model.</td>
	</tr>
	<tr>
		<td>learning rate</td>
		<td>\(3.0\times 10^{-4}\)</td>
		<td>Initial learning rate for training.</td>
	</tr>
	<tr>
		<td>max steps</td>
		<td>\(2\times 10^7\) \(m/s\)</td>
		<td>Number of total simulation steps (actions) taken for training.</td>
	</tr>
	<tr>
		<td>num epochs</td>
		<td>\(5\)</td>
		<td>Number of times each collected observation is used for training.</td>
	</tr>
	<tr>
		<td>time horizon</td>
		<td>\(1000\)</td>
		<td>Horizon for learning, it represents how far in time steps one action can influence a past reward.</td>
	</tr>
	<tr>
		<td>gamma</td>
		<td>\(0.995\)</td>
		<td>Discount factor, it represents how much of a n-future reward (\(R_n\)) is assigned to a present action in the form \(\gamma^n R_n\).</td>
	</tr>
	<tr>
		<td>Curiosity strength</td>
		<td>\(0.1\)</td>
		<td>Strength of the curiosity intrinsic reward signal.</td>
	</tr>
	<tr>
		<td>Curiosity gamma</td>
		<td>\(0.99\)</td>
		<td>Discount factor for the curiosity reward.</td>
	</tr>
	<tr>
		<td>Visual encoding type</td>
		<td>\(\text{nature_cnn}\)</td>
		<td>Type of architecture used for the convolutional layers for the visual observation encoding.</td>
	</tr>
</table>
</p>

<h4 id>Evolution</h4>

<p>The following figure shows the evolution of the training from various configurations. The reward was normalized because each environment had a different reward system and can't be compared directly. Also this figure intends to show the convergence and evolution of the training procedure, while the <a href="#results">results section</a> presents specific rewards for each configuration in a standardized test scene.</p>

<div class="image fit" style="text-align: center">
<img = src="images/training-stats.png">Reward evolution during training. The naming scheme is as follow: 
</br><i>puppo</i> stands for vector observation (like the puppo example), 
</br><i>vis</i> stands for visual observation, 
</br><i>cont</i>  stands for continuous actions, 
</br><i>disc</i>  stands for discrete actions, 
</br><i>vec-comp</i>  stands for vector per action reward (complete), 
</br><i>vec-sparse</i>  stands for vector sparse reward (partial),
</br><i>multiple</i>  stands for a scene with multiple collectibles.</img>
</div>


<h4 id="results">Results</h4>

<p>The next table contains the result of the model trained on the test scene containing <code>#Test Env</code> collectibles which randomly re-spawn when collected. The evaluation metric is the <i>Total Reward</i>, (the number of objects collected over all episodes) and <i>Reset</i>, (the number of times the agent was reset due to leaving the training area). It ran for \(200\) episodes or \(10^6\) steps, (each episode is \(5000\) steps long).</p>

<table>
<tr><th colspan="10" style="text-align: center;"><font size="+2">Experiment Results</font></th></tr>
<tr>
	<th colspan="5" style="text-align: center;">Training</th>
	<th colspan="5" style="text-align: center;">Testing</th>
</tr>
<tr>
	<th>#</th>
	<th>Observation Type</th>
	<th>Action Type</th>
	<th>Train Env</th>
	<th style="border-right: 1px solid;">Reward Type</th>
	<th>Test Env</th>
	<th>Total Reward</th>
	<th>Reset</th>
</tr>
<tr>
	<td>1</td>
	<td>Vector</td>
	<td>Discrete Masked</td>
	<td>1, box</td>
	<td style="border-right: 1px solid;">Per action</td>
	<td>1, box</td>
	<td> \(1027\)</td>
	<td> \(61\)</td>
</tr>
<tr>
	<td>2</td>
	<td>Vector</td>
	<td>Continuous Masked</td>
	<td>1, box</td>
	<td style="border-right: 1px solid;">Per action</td>
	<td>1, box </td>
	<td>\(2324\)</td>
	<td> \(82\)</td>
</tr>
<tr>
	<td>3</td>
	<td>Vector</td>
	<td>Discrete Masked</td>
	<td>1, box</td>
	<td style="border-right: 1px solid;">Sparse</td>
	<td>1, box</td>
	<td>\(4\)</td>
	<td> \(670\)</td>
</tr>
<tr>
	<td>4</td>
	<td>Vector</td>
	<td>Continuous Masked</td>
	<td>1, box</td>
	<td style="border-right: 1px solid;">Sparse</td>
	<td>1, box</td>
	<td>\(0\)</td>
	<td> \(0\)</td>
</tr>
<tr>
	<td>5</td>
	<td>Visual</td>
	<td>Discrete Masked</td>
	<td>1, box</td>
	<td style="border-right: 1px solid;">Per action</td>
	<td>1, box</td>
	<td>\(1370\)</td>
	<td> \(7\)</td>
</tr>
<tr>
	<td>6</td>
	<td>Visual</td>
	<td>Continuous Masked</td>
	<td>1, box</td>
	<td style="border-right: 1px solid;">Per action</td>
	<td>1, box</td>
	<td>\(2883\)</td>
	<td> \(0\)</td>
</tr>
<tr>
	<td>7</td>
	<td>Visual</td>
	<td>Continuous Masked</td>
	<td>24, boxes</td>
	<td style="border-right: 1px solid;">Sparse</td>
	<td>1, boxes</td>
	<td>\(12\)</td>
	<td> \(0\)</td>
</tr>
<tr>
	<td>8</td>
	<td>Visual</td>
	<td>Continuous Masked</td>
	<td>24, boxes</td>
	<td style="border-right: 1px solid;">Sparse</td>
	<td>24, boxes</td>
	<td>\(7119\)</td>
	<td> \(3\)</td>
</tr>
<tr>
	<td>9</td>
	<td>Visual</td>
	<td>Continuous Masked</td>
	<td>1, box</td>
	<td style="border-right: 1px solid;">Per action</td>
	<td>24, boxes</td>
	<td>\(6163\)</td>
	<td> \(0\)</td>
</tr>
</table>

<h4>Environment Variation</h4>

<p>Another interesting setup is varying the size of the environment and checking how it affects the agent's behavior. For this experiment, the environment was scaled down by a factor of \(0.5\) and now having a \(50 \times 50\) \(\text{m}\) area.</p>

<table>
<tr><th colspan="10" style="text-align: center;"><font size="+2">Experiment 2 Results</font></th></tr>
<tr>
	<th colspan="5" style="text-align: center;">Training</th>
	<th colspan="5" style="text-align: center;">Testing</th>
</tr>
<tr>
	<th>#</th>
	<th>Observation Type</th>
	<th>Action Type</th>
	<th>Train Env</th>
	<th style="border-right: 1px solid;">Reward Type</th>
	<th>Test Env</th>
	<th>Total Reward</th>
	<th>Reset</th>
</tr>
<tr>
	<td>9</td>
	<td>Visual</td>
	<td>Continuous Masked</td>
	<td>1, box</td>
	<td style="border-right: 1px solid;">Per action</td>
	<td>1, box</td>
	<td> \(12921\)</td>
	<td> \(249\)</td>
</tr>
<tr>
	<td>10</td>
	<td>Visual</td>
	<td>Continuous Masked</td>
	<td>24, box</td>
	<td style="border-right: 1px solid;">Sparse</td>
	<td>1, box</td>
	<td> \(9219\)</td>
	<td> \(581\)</td>
</tr>
</table>




<h4>Discussion</h4>

TODO- videos + images

<h5>Effect of various action branches at the same time</h5>

<p>One difficult when training with Un-masked actions was that it would not learn anything. Starting from a random policy, the agent would be stuck alternating between forward and backward. This problem with the Y-Axis movement is probably due to the interval between the actions being short and not allowing the agent to commit to a give action for enough time until a strong reward signal were obtained.</p>

<p>Nevertheless increasing the action interval would lead to unwanted delays in the reaction time of the agent. Four solutions worked well:
<ul>
	<li>Giving a slightly reward for moving forward.</li>
	<li>Adding a bias in the actions favoring the forward movement.</li>
	<li>Training the action's branch separated.</li>
	<li>Restricting the Y-Axis to forward movements only.</li>
</ul>
The last one were chosen, given the agent would not need to use backward movement at all in the proposed environment.
</p>

<h5>Visual versus vector observation</h5>

<p>Visual observation achieved better results than vector observation when using the same reward system. It also could learn faster in the environment with many collectibles, while vector observation can't be applied. Despite it requiring more computational power, it shows that having a end-to-end learning may achieve better performance than having hand crafted features</p>

<h5>Discrete versus continuous action space</h5>

<p>
While the continuous actions space seems to be better than the discrete action space in all results, it can't be conclusive. One point is the batch size for continuous and discrete action space were different, (the later being smaller) and they were not extensively searched for the best result.</p>
<p> Even so, it is recommended that continuous actions to have a higher batch size than discrete actions. As the name suggests, continuous actions are continuous and the average between samples makes sense (exists), also another way to think about it is their values are calculated in the form of a sample with a given mean and variance, hence the need to take those from various steps. In the other hand, discrete action are like classification or labeling, so averaging between two or many different labels may not make sense at all. 
</p>

<h5>Per action versus sparse reward</h5>

<p>Using a per-action reward strategy worked in all cases, while sparse reward only worked in the scene with multiple objects. It is clear that even with the drawback of the agent possible exploiting the reward instead of the true objective a per-action strategy is more reliable.</p>
<p>In the other hand the sparse reward, given only when the objects is achieve, can't be exploited, but makes the learning process entirely dependent of the difficult of the environment. For those cases a good approach is using a curriculum, starting with very easy tasks and increasing the difficult according with the agent performance. Nevertheless developing a good curriculum and environments are not easy.</p>

<h5>Single versus multiple objects</h5>

<p>Having multiple objects in the scene completely changed the result of using sparse reward, from unable to learn anything to achieving the best result. Giving the agent an environment where it can often achieve a goal even with a random policy is the key point to use sparse reward. It is a good strategy to be used with curriculum learning, in this case, the difficult could be easily controlled by the amount of collectibles. While it was not test here, one can believe that if the agent was trained with such curriculum it would perform extremely well.</p>

<h5>Environment variation</h5>

<p>Two variations where tested, changing the size of the environment and the number of collectibles. Those variations shown interesting behaviors.</p>
<p>First the agent trained with multiple collectibles which was not able to perform in the single collectible environment, had a good performance when the size of the environment was reduced. From visual inspection, what was happening was: the agent had a short sight. Again if the proposed curriculum was applied this behavior would probably be solved.</p>

<p>Then, the agent trained with a single object performed very well with multiple collectibles, but not as good as the one trained with many objects. Also from visual inspection, the agent would only turn to the right, which indeed works but is not the optimal behavior. In the case of a single object which most of times was far away it would not change much, but in the case of multiple objects it was a limiting factor.</p>

<p>These behaviors reinforces the idea of the need for variation in the training and probably the use of a curriculum of increasing difficult and variability. It also shows the importance of the details of the environment as a key factor for achieving the desired behavior.</p>

<h5>Learning generalization</h5>

<p>Although the agent could somewhat generalize well to multiple and single objects it wasn't as good as the performance of the trained environment. When running on a smaller area it could not cope with the delimited marks and the agent left the are much more than when using the original size.</p>

<p>For achieving good generalization it must experience great variation of the environment. It is a must to provide variations for all aspects of the environment and goal, but following the principles of a curriculum, or the agent may not learn anything.</p>

<!--<h5>(TODO)Camera movement setup and attention like mechanism</h5>-->


<h4>Conclusion</h4>

</p>The key feature to develop is certainly the reward system. Making the agent to be able to have feedback even on a completely random behavior is a must, be it through a per action reward or a easier environment where rewards are not much sparse. Even so, it may get stuck if the action space is complex or if there is not enough time to commit to an action. This was exemplified by the lack of convergence when using the full action space despite having a per action reward. Nevertheless, with sparse reward, but increased amount of objects in the scene lead the agent to converge.</p>

<p>Another point was the observation type, while the vector observation had all the data needed to heuristically solve the problem, the visual observation had a better result. It may be due to encoding which is compact and human understandable, but may not be the best for learning. Letting the model extract the features from visual observation is more expensive for training but can achieve better policies and be easily expansible to new environment.</p>

<p>Modeling and training agents to complete tasks, (in the way one would like to), is a complex problem. Here, the neural network architecture and parameters were not evaluated, but the modeling of the environment and its effects. Good policies were learned, although side effects were present which explicits how an agent can exploit unnoticed details of its environment in unpredictable ways for good, bad and ugly things.</p>

<p>Last, there is no formula to achieve good results. Part of it is the art of modeling the system and part is building insight from past failures and adapting, which itself is much the idea of (meta) reinforcement learning.</p>


<figure style="text-align:center;">
<img class="image 25%" src="images/ch-end.jpg">
</figure>


<code>
idea - high level character control
idea - basic learned actions controlled by a bigger "brain"
basic input -  forward, backward, left, right, jump crouch

training
	- collect pink objects
		- binary reward {0,1}
	- reach stand pink pads
		- inside time reward
	- visual observation
	- forward bias
	- continuous action space (-1,1)
		- network output scaling
	- discrete action space {backward, stand, walk, trot, run}, {left, straight, right}
		- forward bias on reward
	- puppo like
		- high level discrete actions
		- vector observations of direction, distance, borders and velocity
	- environment
		- difficult
			- # items
			- size
			- possible actions
	- reward
		- shape behavior
			- binary
			- increasing
			- time penalty
			- action penalty
		- forward bias
		- difficult to attain the desired end result
	- result
		- puppo like
			- works good, "complete" information of ambient
		- visual observation
			- partial information
			- movement tied to vision, lack of attention
			- independent action branches, hard to train together
			- nonoptimal behavior, forward overfit
		- generalization
			- object shape (because of color?)
			- area size
		- TODO - test  visual observations with puppo like rewards.
	- network architecture
	
	
TODO - test with no restart?</br>
</code>	
			
	