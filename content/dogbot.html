<header class="mini">
<h2>DogBot</h2>
<p>Experimenting reinforcement learning on a playable character</p>
</header>

<h3>High-Level character control</h3>

<p>Reinforcement learning have been a trending topic recently with the Deep Learning techniques. Various interesting applications have been made from playing games (Atari, Go, StarCraft, etc) to controlling torque-actuators for motion control in robotic simulations (biped run, robotic hand solving Rubik's cube, etc). Here we want to experiment with high level controls of a character, which already have its animations and controller, to do simple tasks like collect or fetch an object.
</p>

<p>The chosen character is a dog (call it DogBot for now). It can stand, walk, trot, run, jump and crouch. It was trained to select between those actions at each instant \(t_i\) to accomplish a desired task. More details on the basic animations and controls setup can be found <a href="anim.html">here</a>.
</p>

<p>With this setup, the later goal is to have some basic trained behaviors which can be used by a higher level "brain" (either learned or heuristic based) to interact with players in a virtual environment. This approach resembles a hierarchical learning splitting complex behaviors into a set of simple behaviors, but instead of learning every single component, it can use heuristic behaviors and traditional animation and character controller together with learning.</p>

<h3>Environment</h3>

<p>The environment is a Unity scene where the character can act and interact with objects. It consists of a square gray plane \(110\times 110 m\) with a white border of \(1 m\) diameter, which <i>visually</i> delimits the area of interest. While it is possible to walk past this area, if the agent being trained go past it a final state (game over) is reached leading to a reset or restart.</p>

<div class="image fit" style="text-align: center">
<img src="./images/env.png">Top View from environment</img>
</div>

<p>The training area can contain the following objects:
<ul>
	<li> Collectibles:
		<ul>
			<li>(Simple geometry) Cubes with \(1 m\) edges</li>
			<li>(Complex geometry) Coins with \(1.5 m\) diameter</li>
		</ul>
	</li>
	<li>Standing pad: 
		<ul>
			<li>A painted circle on the ground delimiting a region with \(5 m\) diameter.</li>
		</ul>
	</li>
</ul>
One or many of these objects can be found in a specific scenario for training. Their count, positioning and any other relevant detail will be specified for each training section later.
</p>

<div class="image fit" style="text-align: center">
<img src="./images/objects.png" >Scene objects</img>
</div>

<p>In general, the choice for big objects is mainly due to the use with visual observations where the size of input image is down-sampled to \(84\times 84\) pixels and hence the need of it being big enough to appear in the down-sampled input image.</p>

<div class="image fit" style="text-align: center">
<img src="./images/downsampled.png" >(Left) full resolution, (Right) down-sampled visual observation</img>
</div>

</p> One important aspect of the environment for the later goal is that what is seen by the agent, for example a big pink cube, is not necessarily visible or the same seen by a player. Complex behaviors can be created using simple geometries for the agent while complex geometries and totally different context is seen by the player. Therefore, it enables one to use it in interesting ways to create interactions between the agent(the dogBot) and the player(a human being).</p>

<!-- TODO <img src="">Diff views illustration</img> -->

<p>A form of classifying an environment is relative to how observable it is:
	<ul>
		<li>Completely observable: 
			<ul>
				all information from the environment is available.
			</ul>
		</li>
		<li>Partially observable:
			<ul>
				some of the information from the environment is available.
			</ul>
		</li>
	</ul>
This classification is intrinsic related to how the agent senses its surrounding.
One example of a <i>completely observable</i> environment is: </p>
<p>
<blockquote>
Assuming the previous environment without obstacles and with a single collectible object as goal. If the observations are set as the agent position and direction, the direction to target and target position. It is a completely observable configuration in the sense that independent of the agent state (position and direction) it always have complete information  of itself and the target to complete its goal. Indeed one could even write a heuristic to solve this simple task.
</blockquote>
</p>

<p>In the other hand, following the same example, if visual information is used, (i.e, a 2D image from the agent's vision cone) instead of the aforementioned observations, depending on where the agent/target is, it may be partially or not visible at all. In this case the sensing of the agent varies depending of its state and it is always a partial view of the environment which may or not contain relevant information. This differentiation of how the environment is perceived and how much information is available per observation is an important component of learning performance. 
</p>

<h3>Tasks</h3>

<p>The agent task can be put as a mobility task, given a stimulus it needs to move towards the target point. Inside this general task three specialized tasks are implemented:
<ul>
	<li>Collecting an object:
		<ul>reach the object position, when the agent collides with it the object is removed from the scene, i.e., collected.</ul>
	</li>
	<li>Reach and stay: 
		<ul>reach the standing pad and stay inside it.</ul>
	</li>
	<li>Fetch: 
		<ul>reach the object and go back to its initial position. It also can be thought as reaching two objects, for example, the stick and then who threw it.</ul>
	</li>
</ul>

While these can all be cast as essentially the same task, their difference comes of how they are modeled inside the virtual environment and which information is available to the agent to complete them. That said, they are practical examples of mobility tasks.
</p>

<h3>Agent</h3>

<p>The agent is an entity abstraction which is itself a <i>behavior policy</i>. Nevertheless, when referred without a specific policy it can be imagined as an entity with sensors (observations) and actuators (actions) where a policy can be plugged in, in programming it would be equivalent to an <i>interface</i>. The agent(DogBot) had its actions already briefly described with its standard animations and character controller. Yet, the most important is what it observes to take actions, how both the observations and actions are encoded and how the associated rewards are distributed.</p>

<h4>Character controller</h4>

<p>The Unity character controller is the lower level hierarchy of actions and can be considered as the intrinsic of the agent's actuators. It controls the velocity, turn speed, gravity and other effects of the actuator (the character body properties). The list of the parameters used in the training are in the <a href="#char-table">experiments section</a>, while in the following there is a figure of those parameters inside Unity.</p>

<div class="image 50%" style="display: block; text-align: center; margin-left: auto; margin-right: auto;">
	<img src="./images/char-control.png" style="margin-left: auto; margin-right: auto;">Character controller parameters</img>
</div>

<h4>Observations</h4>

<p>An observation is any kind of sensing of the agent and environment state. Those need to be encoded by numbers that serves as input to the behavior policy which output actions.</p>

<p>The observations and its encoding are the following:
<ul>
	<li>Vector observation: complete observable, hand-crafted features.
		<ul>
			<li>Normalized direction to target: \( d_{\text{target}} = (x,y,z), \quad \lVert d_{\text{target}} \rVert_{2} = 1\)</li>
			<li>Normalized distance to border: \(d_{\text{border}} = (x,y)\), \(\lVert d_{\text{border}} \rVert_{\infty} < 1 \rightarrow \text{inside}, \quad \lVert d_{\text{border}}\rVert_{\infty} \geq 1 \rightarrow \text{outside}\)</li>
			<li>Linear velocity: \( v_{\text{linear}} = (x,y,z) \text{m/s}\)</li>
			<li>Angular velocity: \( v_{\text{angular}} = (x, y, z) \text{rad/s}\)</li>
			<li>Normalized agent forward direction: \( d_{\text{forward}} = (x, y, z), \quad \lVert d_{\text{forward}} \rVert_{2} = 1\)</li>
			<li>Normalized agent up direction: \( d_{\text{up}} = (x, y, z), \quad \lVert d_{\text{up}} \rVert_{2} = 1\)</li>
			<li>Agent local position (agent's position referent to its transformation matrix): \( p_{\text{local}} = (x,y,z) \)
		</ul>
	</li>
	<li>Visual observation: partially observable, down-sampled from the original rendered image, 3rd-person-like camera aligned with agent forward direction.
		<ul>
			<li>2D image:  matrix \(I_{84\times 84}(r,g,b)\)</li>
		</ul>
	</li>
	<li>Mixed observation: partially observable, down-sampled from the original rendered image, 3rd-person-like camera possible unaligned with agent forward direction.
		<ul>
			<li>2D image:  matrix \(I_{84\times 84}(r,g,b)\)</li>
			<li>Local normalized camera forward direction: \( d_{\text{camera}} = (x,y,z), \quad \lVert d_{\text{target}} \rVert_{2} = 1\)
		</ul>
	</li>
</ul>
</p>

<p><i>Note that being completely or partially observable is not bind to the kind of encoding but the information passed. For example, in a 2D chess game, if the observation encoding were a view-from-top 2D image of the entire board, it would be completely observable.</i></p>

<div class="image fit" style="text-align: center">
<img = src="./images/blackbox-diag.svg">Abstraction of observation (input), action (output), model (policy) and the agent.</img>
</div>

<p>The first encoding takes a total of \(20\) numbers (floats) as observation. These were calculated from the agent/environment state and are much like the observations taken by <a href="https://blogs.unity3d.com/pt/2018/10/02/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit/">Puppo, the Corgi</a>, which is a demo made by Unity. There are some differences to fit our modeling. First, the <i>Normalized distance to the border</i> is added, because it is relevant in an unbounded environment to keep the agent inside the desired area. Last all the joint angles and torque information were removed. Puppo works on a low-level control of the joint angles and torque while this new agent works on a high-level control with animations. Those differences are related to <i>how the agent senses itself</i>, while how it senses the rest of the environment and its target remained the same.</p>

<p>Next, the visual encoding consists of \( 84\times 84 \times 3 = 21168\) numbers (floats), in a 2D RGB image. This image is the direct rendering of the agent's view. This kind of observation is interesting for many real world applications. yet it is much more complex and less complete than the first encoding in \(20\) numbers because only partial information can be inferred from it. In spite of that, it can be more general in some aspects. For example if a variable number of collectibles is admitted in the environment, the first encoding would not be able to handle that variation, but using a 2D image the agent could deal with many objects (because the image is already a partial view of the environment).</p>

<p>Finally, the third encoding adds freedom to the camera forward look direction and hence this information is passed with a 3D vector (local camera direction) together with the 2D RGB image.</p>

<p>One differentiation about those observations is: Despite the 2D image drawbacks, it is an agent self-contained sensing while the vector observation used in the first encoding need access of the underlying environment model to be processed and fed to the agent. The third encoding, regardless of using mixed observations still self-contained.</p>

<h4>Actions</h4>

<p>The DogBot's actuator(s) is a character inside Unity. It is composed of various animations and a controller (with blend trees and alikes) which receives four parameters controlling the X-Axis velocity, the Y-Axis rotation and booleans jump/crouch.</p>

<p>For the agent's action encoding two schemes were used continuous and discrete:
<ul>
	<li>Continuous action space:
		<ul>
			<li>Forward and backward movement: \(\in [-1,1]\)</li>
			<li>Steering left and right: \(\in [-1,1]\)</li>
			<li>Jump: \(j \in [-1,1], \quad j > j_0\)</li>
			<li>Crouch: \(c \in [-1,1], \quad c > c_0\)</li>
		</ul>
	</li>
	<li>Discrete action space:
		<ul>
			<li>Forward and backward movement: \(\{\text{backward, none, walk, trot, run}\}\)</li>
			<li>Steering: \(\{\text{left, none, right}\}\)</li>
			<li>Jump: \(\{\text{true, false}\}\)</li>
			<li>Crouch: \(\{\text{true, false}\}\)</li>
		</ul>
	</li>
</ul>
Each bullet is a action branch, and can be choose simultaneously. For the case of Jumping/Crouching, despite being separated branches, priority is given to Jump action over the Crouch as it is not physically possible to do both at the same time. While the encoding for actions are arbitrary, they reflect the ranges of the Unity character controller such as <i>max velocity</i> and <i>turn speed</i>, which are configurable but from the agent's perspective are intrinsic to its actuator.
</p>

<h4>Reward</h4>

<p>The entire universe of reinforcement learning is based on encouraging the best behavior through rewards (much like it is done when teaching a trick to a pet), in other words, rewarding good actions according. It can also be posed as a optimization problem of maximizing the total reward (a short introduction to this matter can be found <a href="./wrap/intro-rl.html">here</a>).</p>

<p>In this scenario two perspectives a brought up: developing a good reward signal is the key to being able to solve this optimization problem, yet most of times it is not as easy to qualify a given action individually, but only the final outcome of a sequence of actions. In theory, even for the cases where only the final outcome is rewarded, in the limit after many (infinite) experiences it would be possible to learn a optimal behavior policy. Sadly in the real world, limited resources are available be it time, number of experiences, and so on. Hence, the art in learning good policies lies in modeling good rewards and good algorithms as much as possible. <i>Here good performance means reduced time and sample complexity.</i></p>


<p>The DogBot agent experiments with  types of reward:
<ul>
	<li>Per action reward:
		<ul>Positive reward is assigned if the agents approximates of the target, negative reward is assigned if it distances itself from it. The formula used in this case is \(r = 0.01 (v_{\text{linear}} \cdot d_{\text{target}})\)</ul>
	</li>
	<li>Sparse reward: 
		<ul>Positive reward \(+1.0\) is given when the agent reach its destination, i.e., the collectible.</ul>
	</li>
</ul>
In all cases, leaving the training area leads to a negative reward of \(-1.0\) ending the episode. Also a small negative reward \(-0.0005\) is given at each time step \(t_i\). It was chosen to be close to \(-\frac{1}{\text{# steps}}\) so the accumulated penalty would not saturate the total reward signal. This is widely used as a time penalty to stimulate the completion of a task in the shortest time.
</p>

<p>It's also important to classify these rewards in another way. The first reward needs a broader knowledge of the environment to be calculated depending both of the agent and environment underlying state. In the other hand, the second could be completely assigned with the agent's sensing. This is interesting because it resembles the agent self-contained property and have implications in the learning performance and generalization. For obvious reasons the later is expected to be harder.</p>

<h3>Training</h3>

<p>The tools used for training were Unity and its machine learning framework <a href="https://unity3d.com/machine-learning/">ML-Agents</a>. All experiments use the Proximal Policy Optimization (<a href="https://openai.com/blog/openai-baselines-ppo/">PPO</a>).</p> 

<p>Each experiment took between \(16\) to \(30\) hours to complete, not accounting for many "pre-experiments" and tests. For this reason, they were run only once and across different machines. While some insight can be taken from the results, it may not be accurate to take conclusions from single experiments even that they usually tend to converge to the same solution across multiple runs.</p>


<p>The ML-Agents framework exposes some parameters related to it's build-in models (network architecture), they will be specified inside the <a href="#exp_sec">experiment section</a>. Other specific details of the network itself are omitted because they are the default of ML-Agents and can be found in their <a href="https://github.com/Unity-Technologies/ml-agents/">documentation and code</a>. <i>This choice was made because the focus is on the development of the agent, environment and reward signal instead of network architecture.</i>
</p>



<h4 id="exp_sec">Experiments</h4>

The first is an implementation that resembles the <a href="https://blogs.unity3d.com/pt/2018/10/02/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit/">Unity Puppo, the Corgi</a> where vector observation with information such as, speed, direction, direction to target and others are used to describe the scene. While the given environment has many differences from the aforementioned, it is a good starting point  to compares results qualitatively. 

<ul>
	<li>Puppo like obs and reward</li>
	<li>Puppo like obs sparse reward</li>
	<li>Vis encode and puppo like reward</li>
	<li>Vis encode sparse reward</li>
	<li>Multiple objects, vis encode, sparse</li>
	<li>Multiple objects, vis encode, sparse, pre-trained puppo like</li>
	<li>Vis encode, Standing pad</li>
	<li>Vis encode, Camera movement detached from forward direction</li>
</ul>

<p>All experiments uses the following parameters:

<table id="char-table">
	<tr><th colspan="3" style="text-align: center">Character Controller</th></tr>
	<tr>
		<th>Name</th>
		<th>Value</th>
		<th>Description</th>
	</tr>
	<tr>
		<td>Moving turn speed</td>
		<td>\(45.0\) \(deg/s\)</td>
		<td>Turn speed when not stationary</td>
		</td>
	</tr>
	<tr>
		<td>Stationary turn speed</td>
		<td>\(30.0\) \(deg/s\)</td>
		<td>Turn speed when stationary</td>
	</tr>
	<tr>
		<td>Jump power</td>
		<td>\(5.0\) \(m/s\)</td>
		<td>Vertical velocity applied when jumping</td>
	</tr>
	<tr>
		<td>Forward Velocity</td>
		<td>\(9.0\) \(m/s\)</td>
		<td>Maximum forward velocity</td>
	</tr>
	<tr>
		<td>Backward Velocity</td>
		<td>\(2.0\) \(m/s\)</td>
		<td>Maximum backward velocity</td>
	</tr>
	<tr>
		<td>Gravity multiplier</td>
		<td>\(1.0\)</td>
		<td>Multiplier for gravity simulation</td>
	</tr>
	<tr>
		<td>Anim speed multiplier</td>
		<td>\(1.0\)</td>
		<td>Multiplier for animation time scale</td>
	</tr>
</table>
</p>

<p>
<table>
	
	<tr><th colspan="3" style="text-align: center">Training Configuration</th></tr>
	<tr>
		<th>Name</th>
		<th>Value</th>
		<th>Description</th>
	</tr>
	<tr>
		<td>buffer size</td>
		<td>\(40960\)</td>
		<td>Number of samples collected for each policy update</td>
		</td>
	</tr>
	<tr>
		<td>hidden units</td>
		<td>\(512\)</td>
		<td>Number of neurons per hidden layer</td>
	</tr>
	<tr>
		<td>num layers</td>
		<td>\(2\)</td>
		<td>Number of hidden layers used for the model</td>
	</tr>
	<tr>
		<td>learning rate</td>
		<td>\(3.0\times 10^{-4}\)</td>
		<td>Initial learning rate for training</td>
	</tr>
	<tr>
		<td>max steps</td>
		<td>\(2\times 10^7\) \(m/s\)</td>
		<td>Number of total simulation steps (actions) taken for training</td>
	</tr>
	<tr>
		<td>num epochs</td>
		<td>\(5\)</td>
		<td>Number of times each collected observation is used for training</td>
	</tr>
	<tr>
		<td>time horizon</td>
		<td>\(1000\)</td>
		<td>Horizon for learning, it represents how far in time steps one action can influence a past reward</td>
	</tr>
	<tr>
		<td>gamma</td>
		<td>\(0.995\)</td>
		<td>Discount factor, it represents how much of a n-future reward (\(R_n\)) is assigned to a present action in the form \(\gamma^n R_n\).</td>
	</tr>
</table>
</p>

<p>The following table present the result of running the trained model on the test scene containing a single collectible which randomly re-spawn when collected. The evaluations metric is the <i>Mean Reward</i>, the average number of objects collected per episode. It was run for \(200\) episodes or \(10^6\) steps, (each episode is \(5000\) steps long).</p>

<p>The <a href="suc_tb">successful result table</a> contains the training which converged, in other words, the ones that learned a working policy.</p>
<table>
<tr><th colspan="6" style="text-align: center">Successful Results</th></tr>
<tr>
	<th>#</th>
	<th>Observation Type</th>
	<th>Action Type</th>
	<th>Reward Type</th>
	<th>Collectible</th>
	<th>Mean Reward</th>
</tr>
<tr>
	<td>1</td>
	<td>Vector</td>
	<td>Discrete Masked</td>
	<td>Per action</td>
	<td>1, box</td>
	<td> \(0.0\)</td>
</tr>
<tr>
	<td>2</td>
	<td>Vector</td>
	<td>Continuous Masked</td>
	<td>Per action</td>
	<td>1, box </td>
	<td>\(0.0\)</td>
</tr>
<tr>
	<td>3</td>
	<td>Vector</td>
	<td>Discrete Masked</td>
	<td>Sparse</td>
	<td>1, box</td>
	<td>\(0.0\)</td>
</tr>
<tr>
	<td>4</td>
	<td>Vector</td>
	<td>Continuous Masked</td>
	<td>Sparse</td>
	<td>1, box</td>
	<td>\(0.0\)</td>
</tr>
<tr>
	<td>5</td>
	<td>Visual</td>
	<td>Discrete Masked</td>
	<td>Per action</td>
	<td>1</td>
	<td>\(0.0\)</td>
</tr>
<tr>
	<td>6</td>
	<td>Visual</td>
	<td>Continuous Masked</td>
	<td>Per action</td>
	<td>1, box</td>
	<td>\(0.0\)</td>
</tr>
<tr>
	<td>7</td>
	<td>Visual</td>
	<td>Continuous Masked</td>
	<td>Sparse</td>
	<td>24, boxes</td>
	<td>\(0.0\)</td>
</tr>
</table>


<p>Now, the <a href="suc_tb">successful result table</a></p>
<table>
<tr><th colspan="6" style="text-align: center">Unsuccessful Results</th></tr>
<tr>
	<th>#</th>
	<th>Observation Type</th>
	<th>Action Type</th>
	<th>Reward Type</th>
	<th>Collectibles</th>
	<th>Mean Reward</th>
</tr>
<tr>
	<td>1</td>
	<td>Vector</td>
	<td>Discrete</td>
	<td>Per action</td>
	<td>1</td>
	<td> \(0.0\)</td>
</tr>
</table>

<p>TODO - results</p>


<h5>Environment Variation</h5>

<p>Two other interesting setup are: the standing pad which has a different objective, testing the model trained with a single collectible against an environment with various collectibles and varying the environment size/scale. This later case is only possible with visual observation, since its input/output are the same independent of the number of objects.</p>

<p>TODO: scores</p>
<p>TODO: # of times agent leaves training area when using variations</p>



<h4>Discussion</h4>

<p>From the various modeling and its results, some guides for modeling the agents sensing, the environment complexity and also the reward signal can be guessed. Here, they are split according with their modeling.</p>

<h5>Effect of various action branches at the same time</h5>
<h5>Visual versus vector observation</h5>
<h5>Discrete versus continuous action space</h5>
<h5>Per action versus sparse reward</h5>
<h5>Single versus multiple objects</h5>
<h5>Camera movement setup and attention like mechanism</h5>


<h4>Conclusion</h4>

<p>Modeling and training agents to complete tasks, (in the way we would like), is a complex problem. Here, the neural network architecture and parameters were not evaluated, but the modeling of the environment with the objective of obtaining useful insight and correlate it to the theory.</p> 
</p>The key feature to develop is certainly the reward system, making the agent being able to have feed back even on a completely random behavior is a must. Even so it may get stuck if the action space is complex. This was exemplified by the lack of convergence when using the full action space despite having a per action reward. Nevertheless, with a masked action space and sparse reward, increasing the amount of objects in the scene lead the agent to converge.</p>


<figure style="text-align:center;">
<img class="image 25%" src="./images/ch-end.jpg">
</figure>


<code>
idea - high level character control
idea - basic learned actions controlled by a bigger "brain"
basic input -  forward, backward, left, right, jump crouch

training
	- collect pink objects
		- binary reward {0,1}
	- reach stand pink pads
		- inside time reward
	- visual observation
	- forward bias
	- continuous action space (-1,1)
		- network output scaling
	- discrete action space {backward, stand, walk, trot, run}, {left, straight, right}
		- forward bias on reward
	- puppo like
		- high level discrete actions
		- vector observations of direction, distance, borders and velocity
	- environment
		- difficult
			- # items
			- size
			- possible actions
	- reward
		- shape behavior
			- binary
			- increasing
			- time penalty
			- action penalty
		- forward bias
		- difficult to attain the desired end result
	- result
		- puppo like
			- works good, "complete" information of ambient
		- visual observation
			- partial information
			- movement tied to vision, lack of attention
			- independent action branches, hard to train together
			- nonoptimal behavior, forward overfit
		- generalization
			- object shape (because of color?)
			- area size
		- TODO - test  visual observations with puppo like rewards.
	- network architecture
	
	
TODO - test with no restart?</br>
</code>	
			
	