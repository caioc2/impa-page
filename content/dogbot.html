<header class="mini">
<h2>DogBot</h2>
<p>Experimenting reinforcement learning on a playable character</p>
</header>

<h3>High-Level character control</h3>

<p>Reinforcement learning have been a trending topic recently with the Deep Learning techniques. Various interesting applications have been made from playing games (Atari, Go, StarCraft, etc) to controlling torque-actuators for motion control in robotic simulations (biped run, robotic hand solving Rubik's cube, etc). Here we want to experiment with high level controls of a character, which already have its animations and controller, to do simple tasks like collect or fetch an object.
</p>

<p>The chosen character is a dog (call it DogBot for now). It can stand, walk, trot, run, jump and crouch. It was trained to select between those actions at each instant \(t_i\) to accomplish a desired task. More details on the basic animations and controls setup can be found <a href="anim.html">here</a>.
</p>

<p>With this setup, the later goal is to have some basic trained behaviors which can be used by a higher level "brain" (either learned or heuristic based) to interact with players in a virtual environment. This approach resembles a hierarchical learning splitting complex behaviors into a set of simple behaviors, but instead of learning every single component, it can use heuristic behaviors and traditional animation and character controller together with learning.</p>

<h3>Environment</h3>

<p>The environment is a Unity scene where the character can act and interact with objects. It consists of a square gray plane \(110\times 110 m\) with a white border of \(1 m\) diameter, which <i>visually</i> delimits the area of interest. While it is possible to walk past this area, if the agent being trained go past it a final state (game over) is reached leading to a reset or restart.</p>

<p>The training area can contain the following objects:
<ul>
	<li> Collectibles:
		<ul>
			<li>(Simple geometry) Cubes with \(1 m\) edges</li>
			<li>(Complex geometry) Coins with \(1.5 m\) diameter</li>
		</ul>
	</li>
	<li>Standing pad: a painted circle on the ground delimiting a region with \(5 m\) diameter.</li>
</ul>
One or many of these objects can be found in a specific scenario for training. Their count, positioning and any other relevant detail will be specified for each training section later.
</p>

<p>In general, the choice for big objects is mainly due to the use with visual observations where the size of input image is down-sampled to \(84\times 84\) pixels and hence the need of it being big enough to appear in the down-sampled input image.</p>

</p> One important aspect of the environment for the later goal is that what is seen by the agent, for example a big pink cube, is not necessarily visible or the same seen by a player. Complex behaviors can be created using simple geometries for the agent while complex geometries and totally different context is seen by the player. Therefore, it enables one to use it in interesting ways to create interactions between the agent(the dogBot) and the player(a human being).</p>

<p>A characteristic of any environment is how observable it is, and it's intrinsic related to how the agent senses its surrounding. A high level differentiation with respect to that is:
	<ul>
		<li>Completely observable: all information from the environment is available.</li>
		<li>Partially observable: some of the information from the environment is available.</li>
	</ul>
One example as where it is called <i>completely observable</i>: </p>
<p>
<blockquote>
Assuming the previous environment without obstacles and with a single collectible object as goal. If the observations are set as the agent position and direction, the direction to target and target position. It is a completely observable configuration in the sense that independent of the agent state (position and direction) it always have complete information  of itself and the target to complete its goal. Indeed one could even write a heuristic to solve this simple task.
</blockquote>
</p>

<p>In the other hand, following the same example, if visual information is used, (i.e, a 2D image from the agent's vision cone) instead of the aforementioned observations, depending on where the agent/target is, it may be partially or not visible to each other at all. In this case the sensing of the agent varies depending of its state and it is always partial. This differentiation of how the environment is perceived and how much information is available per observation dictates the performance of learning. 
</p>

<h3>Tasks</h3>

<p>The agent task can be put as a mobility task, given a stimulus it needs to move towards the target point. Inside this general task three specialized tasks are implemented:
<ul>
	<li>Collecting an object: reach the object position, when the agent collides with it the object is removed from the scene, i.e., collected.</li>
	<li>Reach and stay: reach the standing pad and stay inside it.</li>
	<li>Fetch: reach the object and go back to its initial position. It also can be thought as reaching two objects, for example, the stick and then who threw it.</li>
</ul>

While these can all be cast as essentially the same task, their difference comes of how they are modeled inside the virtual environment and which information is available to the agent to complete them. Nevertheless, they are practical examples of mobility tasks.
</p>

<h3>Agent</h3>

<p>The agent(DogBot) had its actions already briefly described with its standard animations and character controller. Yet, the most important is what it observes to take actions and how both the observations and actions are encoded.</p>

<p>The encoding used for the observations follow different paradigms. They are:
<ul>
	<li>Vector observation: complete observable, hand-crafted features.
		<ul>
			<li>Normalized direction to target: \( d_{\text{target}} = (x,y,z), \quad \lVert d_{\text{target}} \rVert_{2} = 1\)</li>
			<li>Normalized distance to border: \(d_{\text{border}} = (x,y)\), \(\lVert d_{\text{border}} \rVert_{\infty} < 1 \rightarrow \text{inside}, \quad \lVert d_{\text{border}}\rVert_{\infty} \geq 1 \rightarrow \text{outside}\)</li>
			<li>Linear velocity: \( v_{\text{linear}} = (x,y,z) \text{m/s}\)</li>
			<li>Angular velocity: \( v_{\text{angular}} = (x, y, z) \text{rad/s}\)</li>
			<li>Normalized agent forward direction: \( d_{\text{forward}} = (x, y, z), \quad \lVert d_{\text{forward}} \rVert_{2} = 1\)</li>
			<li>Normalized agent up direction: \( d_{\text{up}} = (x, y, z), \quad \lVert d_{\text{up}} \rVert_{2} = 1\)</li>
			<li>Agent local position (agent's position referent to its transformation matrix): \( p_{\text{local}} = (x,y,z) \)
		</ul>
	</li>
	<li>Visual observation: partially observable, down-sampled from the original rendered image, 3rd-person-like camera aligned with agent forward direction.
		<ul>
			<li>2D image:  matrix \(I_{84\times 84}(r,g,b)\)</li>
		</ul>
	</li>
	<li>Mixed observation: partially observable, down-sampled from the original rendered image, 3rd-person-like camera possible unaligned with agent forward direction.
		<ul>
			<li>2D image:  matrix \(I_{84\times 84}(r,g,b)\)</li>
			<li>Local normalized camera forward direction: \( d_{\text{camera}} = (x,y,z), \quad \lVert d_{\text{target}} \rVert_{2} = 1\)
		</ul>
	</li>
</ul>
</p>

<p><i>Note that being completely or partially observable is not bind to the kind of encoding but the information passed. For example, in a 2D chess game, if the observation encoding were a view-from-top 2D image of the entire board, it would be completely observable.</i></p>

<p>The first encoding takes a total of \(20\) numbers (floats) as observation. These were calculated from the agent/environment state and are much like the observations taken by <a href="https://blogs.unity3d.com/pt/2018/10/02/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit/">Puppo, the Corgi</a>, which is a demo made by Unity. There are some differences to fit our modeling. First, the <i>Normalized distance to the border</i> is added, because it is relevant in an unbounded environment to keep the agent inside the desired area. Last all the joint angles and torque information was removed. Puppo works on a low-level control of the joint angles and torque while this new agent works on a high-level control with animations. Those differences are related to <i>how the agent senses itself</i>, while how it senses the rest of the environment and its target remained the same.</p>

<p>Next, the visual encoding consists of \( 84\times 84 \times 3 = 21168\) numbers (floats), in a 2D RGB image. This image is the direct rendering of the agent's view. This kind of observation is interesting for many real world applications. yet it is much more complex and less complete than the first encoding in \(20\) numbers because only partial information can be inferred from it. In spite of that, it can be more general in some aspects. For example if a variable number of collectibles is admitted in the environment, the first encoding would not be able to handle that variation, but using a 2D image the agent could deal with many objects (because the image is already a partial view of the environment).</p>

<p>Finally, the third encoding adds freedom to the camera forward look direction and hence this information is passed with a 3D vector (local camera direction) together with the 2D RGB image.</p>

<p>One differentiation about those observations is: Despite the 2D image drawbacks, it is an agent self-contained sensing while the vector observation used in the first encoding need access of the underlying environment model to be processed and fed to the agent. The third encoding, regardless of using a mixed observations stills self-contained.</p>



<p>For the agent's action encoding two schemes were used continuous and discrete:
<ul>
	<li>Continuous action space:
		<ul>
			<li>Forward and backward movement: \(\in [-1,1]\)</li>
			<li>Steering left and right: \(\in [-1,1]\)</li>
			<li>Jump: \(j \in [-1,1], \quad j > \text{threshold}_j\)</li>
			<li>Crouch: \(c \in [-1,1], \quad c > \text{threshold}_c\)</li>
		</ul>
	</li>
	<li>Discrete action space:
		<ul>
			<li>Forward and backward movement: \(\{\text{backward, none, walk, trot, run}\}\)</li>
			<li>Steering: \(\{\text{left, none, right}\}\)</li>
			<li>Jump: \(\{\text{true, false}\}\)</li>
			<li>Crouch: \(\{\text{true, false}\}\)</li>
		</ul>
	</li>
</ul>
Each bullet is a action branch, and can be choose simultaneously. For the case of Jumping/Crouching, despite being separated branches, priority is given to Jump action over the Crouch as it is not physically possible to do both at the same time.
</p>

<h3>Training</h3>

TODO: mlagents setup etc

<h4>Experiments</h4>

The first is an implementation that resembles the <a href="https://blogs.unity3d.com/pt/2018/10/02/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit/">Unity Puppo, the Corgi</a> where vector observation with information such as, speed, direction, direction to target and others are used to describe the scene.





<figure style="text-align:center;">
<img class="image 25%" src="../images/ch-end.jpg">
</figure>


idea - high level character control
idea - basic learned actions controlled by a bigger "brain"
basic input -  forward, backward, left, right, jump crouch

training
	- collect pink objects
		- binary reward {0,1}
	- reach stand pink pads
		- inside time reward
	- visual observation
	- forward bias
	- continuous action space (-1,1)
		- network output scaling
	- discrete action space {backward, stand, walk, trot, run}, {left, straight, right}
		- forward bias on reward
	- puppo like
		- high level discrete actions
		- vector observations of direction, distance, borders and velocity
	- environment
		- difficult
			- # items
			- size
			- possible actions
	- reward
		- shape behavior
			- binary
			- increasing
			- time penalty
			- action penalty
		- forward bias
		- difficult to attain the desired end result
	- result
		- puppo like
			- works good, "complete" information of ambient
		- visual observation
			- partial information
			- movement tied to vision, lack of attention
			- independent action branches, hard to train together
			- nonoptimal behavior, forward overfit
		- generalization
			- object shape (because of color?)
			- area size
	- network archtecture
			
	